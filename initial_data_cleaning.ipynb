{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Initial Data Cleaning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np, pandas as pd, os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size = 4>1. Read in the data files</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This variable should be either 11 or 12 for the year 2011 or 2012 respectively\n",
      "YEAR = 12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "accidents = pd.read_table(os.path.join(\"GES_{}\".format(YEAR), \"ACCIDENT.TXT\"))\n",
      "vehicles = pd.read_table(os.path.join(\"GES_{}\".format(YEAR), \"VEHICLE.TXT\"))\n",
      "persons = pd.read_table(os.path.join(\"GES_{}\".format(YEAR), \"PERSON.TXT\"))\n",
      "safetyEQ = pd.read_table(os.path.join(\"GES_{}\".format(YEAR), \"SAFETYEQ.TXT\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>2. Filter out the PERSON records to retain only those records of cyclists who had an evident and non-incapacitating, incapacitating, or fatal injury</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cond1 = persons.PER_TYP == 6 #person type == \"bicyclist\"\n",
      "cond2 = persons.PER_TYP == 7 #person type == \"other cyclist\"\n",
      "cond3 = persons.INJ_SEV.isin([2,3,4]) #injury severity is evident and non-incapacitating, incapacitating, or fatal\n",
      "cond4 = persons.INJSEV_IM.isin([2,3,4]) #imputed injury severity is evident and non-incapacitating, incapacitating, or fatal\n",
      "cyclists = persons[(cond1 | cond2) & (cond3 | cond4)] #cyclists with evident&non-incapacitating, incapacitating, or fatal injuries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>3. Merge the GES VEHICLE data and ACCIDENT data with the PERSONS data for each injured cyclist</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Augment the person data on the cyclists with the vehicle data for the vehicle which struck the cyclist\n",
      "vehMerged = pd.merge(cyclists, vehicles, how=\"left\", left_on = [\"CASENUM\", \"STR_VEH\"],\n",
      "                     right_on = [\"CASENUM\", \"VEH_NO\"], suffixes = (\"\", \"_vehDup\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Augment the person and vehicle data with accident data\n",
      "fullyMerged = pd.merge(vehMerged, accidents, how = \"left\", on = \"CASENUM\", suffixes = (\"\", \"_accDup\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>4. Remove duplicates in the newly merged dataframe</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def removeDuplicates(df):\n",
      "    from pandas.util.testing import assert_series_equal\n",
      "    \n",
      "    unequalList = [] #Initialize a list to store the duplicate columns with new information\n",
      "    dups = [] #Initialize a list to store the duplicate columns\n",
      "    for col in df.columns: #Iterate over all of the columns\n",
      "        if \"_vehDup\" in col or \"_accDup\" in col: #Check if the column is a duplicate column\n",
      "            dups.append(col) #Add the column name to the list of duplicates\n",
      "            orig = col[:-7] #Isolate the original column name of the duplicate\n",
      "            try: assert_series_equal(df[col], df[orig]) #See if the duplicate column and the original column are equal\n",
      "            except:\n",
      "                unequalList.append(col) #If the original and duplicate column are not equal, add the duplicate to the unequal list\n",
      "    equalDups = list(set(dups).difference(set(unequalList))) #Isolate the duplicate columns which are equal\n",
      "    df.drop(equalDups, axis = 1, inplace = True) #Drop the equal duplicate columns from df. Note that here axis = 1 for columns\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "removeDuplicates(fullyMerged) #Remove duplicate columns which contain no new information"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>5. Create new columns containing information about the driver which struck the cyclist</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeNewColumns(df, columnList, initialVals):\n",
      "    \"\"\"df = the dataframe to be altered.\n",
      "    columnList = a list of the new columns to be added to the dataframe\n",
      "    initialVals = a list of the initial values for the new columns.\n",
      "    initialVals should be of length 1 or of length == len(columnList).\n",
      "    If len(initialVals) == 1 then all columns will have the same initial value.\"\"\"\n",
      "    \n",
      "    if len(initialVals) == len(columnList): #Create the tuples of column and inital value pairs\n",
      "        iterable = zip(columnList,initialVals)\n",
      "    elif len(initialVals) == 1: #Create the tuples of column and inital value pairs\n",
      "        val = initialVals[0] #Use the sole value in initialVals.\n",
      "        iterable = [(x, val) for x in columnList]\n",
      "    for i, j in iterable:\n",
      "        df[i] = j #Create and initialize the new columns\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fillNewColumns(ser, people):\n",
      "    \"\"\"ser = a row series in the data frame the new column is being made for\n",
      "    people = the dataframe of the GES person data\n",
      "    \n",
      "    ========\n",
      "    Fills in the new columns of ser using the people dataframe.\n",
      "    Note this function is \"hard-coded\" given the columns created with makeNewColumns().\"\"\"\n",
      "    relPpl = persons[persons.CASENUM == ser[\"CASENUM\"]] #Filter the person file to only those involved in the relevant crash\n",
      "    strikingDriver = relPpl[(relPpl.VEH_NO == ser[\"STR_VEH\"]) & (relPpl.PER_TYP == 1)] #Isolate the driver who struck the cyclist\n",
      "    if len(strikingDriver) > 0: #Make sure the vehicle has a driver. One record does not have a driver.\n",
      "        if strikingDriver[\"AGE\"].iloc[0] in [998, 999]: #Check if the age is unknown\n",
      "            ser[\"DRIVER_AGE\"] = strikingDriver[\"AGE_IM\"].iloc[0] #If the age is unknown, use the imputed age\n",
      "        else:\n",
      "            ser[\"DRIVER_AGE\"] = strikingDriver[\"AGE\"].iloc[0] #Get the striking driver's age\n",
      "        if strikingDriver[\"SEX\"].iloc[0] in [8, 9]: #Check if the sex is unknown\n",
      "            ser[\"DRIVER_SEX\"] = strikingDriver[\"SEX_IM\"].iloc[0] #If the sex is unknown, use the imputed sex\n",
      "        else:\n",
      "            ser[\"DRIVER_SEX\"] = strikingDriver[\"SEX\"].iloc[0] #Get the striking driver's sex\n",
      "        if strikingDriver[\"DRUGS\"].iloc[0] not in [8, 9]: #Check if the striking driver's drug use is known\n",
      "            ser[\"DRIVER_DRUGS\"] = strikingDriver[\"DRUGS\"].iloc[0] #If so, use the drug report\n",
      "    else:\n",
      "        ser[\"DRIVER_AGE\"] = 9999 #Say you don't know the driver's age\n",
      "        ser[\"DRIVER_SEX\"] = 9999 #Say you don't know the driver's sex\n",
      "        ser[\"DRIVER_DRUGS\"] = 9999 #Say you don't know the driver's drug status\n",
      "    return ser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "newCols = [\"DRIVER_AGE\", \"DRIVER_SEX\", \"DRIVER_DRUGS\"] #Create a list of the new columns to be added to fullyMerged\n",
      "\n",
      "makeNewColumns(fullyMerged, newCols, [9999]) #Initialize the new columns in fullyMerged\n",
      "\n",
      "fullyMerged = fullyMerged.apply(fillNewColumns, axis = 1, args=(persons,)) #Fill in the new columns in fullyMerged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>6. Remove undesired or unhelpful (i.e. low-entropy) columns</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Output a csv, which I then use to get the column names in one column. Next to it I place a column called \"Include\" which contains\n",
      "#0 if I don't think the field is useful and 1 if I do think the field is useful. I saved that csv as \"fullyMerged_usefulVars.csv\"\n",
      "fullyMerged.to_csv(\"fullyMerged_20{}.csv\".format(YEAR))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calcEntropy(df, col):\n",
      "    \"\"\"df = the dataframe the entropy will be calculated based on\n",
      "    col = the column that the entropy calculation will be based on\"\"\"\n",
      "    total = len(df.index) #Get the total number of rows in the dataframe\n",
      "    counts = df.groupby(col).size() #Get a dictionary of counts per value for the given column in df.\n",
      "    probs = [float(x)/total for x in counts.values] #Calculate probabilities for each value in the dataframe\n",
      "    log2 = lambda x: np.log(x)/np.log(2) #Create a function to calculate log-base-2 of a number\n",
      "    tot = 0 #Initialize the entropy value\n",
      "    for prob in probs:\n",
      "        tot += prob * log2(prob) #Iterate over each value in the column, adding p * log_2(p) to the total\n",
      "    return -1 * tot #Multiply the total value by negative one to get the entropy value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def removeUnhelpfulColumns(df, filepath, vLimit):\n",
      "    \"\"\"df = the dataframe to remove unhelpful columns from\n",
      "    filepath = a path to a csv file with columns: \"Cols\" and \"Include\" with a 1 in \"Include for columns to include \n",
      "    and 0 for columns to exclude\n",
      "    vLimit = the lowest acceptable number of observations acceptable in a count of matching observations for a \n",
      "    binary variable.\n",
      "    \n",
      "    ==========\n",
      "    This function Will first remove columns manually decided to be unhelpful. Then, it will go through the remaining columns\n",
      "    and check if the entropy in the column is lower than that in a reference case based on vLimit. Essentially, this removes\n",
      "    columns with low variability within it.\"\"\"\n",
      "    usefulVars = pd.read_csv(filepath) #Read in the csv file which denotes the variables I do and do not want to include\n",
      "    drop = [] #Initialize a variable to hold the variables that will be dropped from the dataframe.\n",
      "    for ind, row in usefulVars.iterrows(): #Iterate over all the columns, adding the ones to be dropped to 'drop'\n",
      "        if row[\"Include\"] == 0:\n",
      "            drop.append(row[\"Cols\"])\n",
      "    df.drop(drop, axis = 1, inplace = True) #Drop the columns which I specifed to drop\n",
      "    log2 = lambda x: np.log(x)/np.log(2) #Create a function to take log_2 of a number\n",
      "    refProbs = [float(vLimit) / len(df), (len(df) - float(vLimit))/len(df)] #Create probabilities for a minimum entropy score\n",
      "    refEntropy = 0 #Initialize the minimum entropy\n",
      "    for p in refProbs:\n",
      "        refEntropy -= p * log2(p) #Iteratively calculate the minimum entropy to which all columns will be compared.\n",
      "    drop = [] #Re-initialize an empty list for the column to be dropped\n",
      "    for col in df.columns: #Iterate over every column in the dataframe\n",
      "        if calcEntropy(df, col) < refEntropy: #Compare the entropy within the column to the minimum entropy value\n",
      "            drop.append(col) #If the entropy of the column < the minimum entropy, add it to the list to be dropped from df.\n",
      "    df.drop(drop, axis = 1, inplace = True) #Drop the columns with too low of an entropy score.\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "removeUnhelpfulColumns(fullyMerged, \"fullyMerged_usefulVars.csv\", 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fullyMerged.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 1919 entries, 0 to 1918\n",
        "Data columns (total 95 columns):\n",
        "CASENUM            1919 non-null int64\n",
        "PER_NO             1919 non-null int64\n",
        "SEX_IM             1919 non-null int64\n",
        "INJSEV_IM          1919 non-null int64\n",
        "PERALCH_IM         1919 non-null int64\n",
        "AGE_IM             1919 non-null int64\n",
        "VE_FORMS           1919 non-null int64\n",
        "WEIGHT             1919 non-null float64\n",
        "REGION             1919 non-null int64\n",
        "STRATUM            1919 non-null int64\n",
        "PJ                 1919 non-null int64\n",
        "PSU                1919 non-null int64\n",
        "PSUSTRAT           1919 non-null int64\n",
        "MONTH              1919 non-null int64\n",
        "HOUR               1919 non-null int64\n",
        "MINUTE             1919 non-null int64\n",
        "AGE                1919 non-null int64\n",
        "SEX                1919 non-null int64\n",
        "INJ_SEV            1919 non-null int64\n",
        "DRINKING           1919 non-null int64\n",
        "DRUGS              1919 non-null int64\n",
        "HOSPITAL           1919 non-null int64\n",
        "P_SF1              1919 non-null int64\n",
        "LOCATION           1919 non-null int64\n",
        "HITRUN_IM          1919 non-null int64\n",
        "BDYTYP_IM          1919 non-null int64\n",
        "IMPACT1_IM         1919 non-null int64\n",
        "PCRASH1_IM         1919 non-null int64\n",
        "V_ALCH_IM          1919 non-null int64\n",
        "NUMOCCS            1919 non-null int64\n",
        "HIT_RUN            1919 non-null int64\n",
        "BODY_TYP_vehDup    1919 non-null int64\n",
        "TOW_VEH_vehDup     1919 non-null int64\n",
        "J_KNIFE            1919 non-null int64\n",
        "GVWR               1919 non-null int64\n",
        "V_CONFIG           1919 non-null int64\n",
        "CARGO_BT           1919 non-null int64\n",
        "BUS_USE            1919 non-null int64\n",
        "SPEC_USE_vehDup    1919 non-null int64\n",
        "EMER_USE_vehDup    1919 non-null int64\n",
        "TRAV_SP            1919 non-null int64\n",
        "IMPACT1_vehDup     1919 non-null int64\n",
        "VEH_SC1            1919 non-null int64\n",
        "VEH_SC2            1919 non-null int64\n",
        "VEH_ALCH           1919 non-null int64\n",
        "SPEEDREL           1919 non-null int64\n",
        "DR_SF1             1919 non-null int64\n",
        "DR_SF2             1919 non-null int64\n",
        "DR_SF3             1919 non-null int64\n",
        "DR_SF4             1919 non-null int64\n",
        "VTRAFWAY           1919 non-null int64\n",
        "VNUM_LAN           1919 non-null int64\n",
        "VSPD_LIM           1919 non-null int64\n",
        "VALIGN             1919 non-null int64\n",
        "VPROFILE           1919 non-null int64\n",
        "VSURCOND           1919 non-null int64\n",
        "VTRAFCON           1919 non-null int64\n",
        "VTCONT_F           1919 non-null int64\n",
        "P_CRASH1           1919 non-null int64\n",
        "P_CRASH2           1919 non-null int64\n",
        "P_CRASH3           1919 non-null int64\n",
        "PCRASH4            1919 non-null int64\n",
        "PCRASH5            1919 non-null int64\n",
        "ACC_TYPE           1919 non-null int64\n",
        "WKDY_IM            1919 non-null int64\n",
        "HOUR_IM            1919 non-null int64\n",
        "MINUTE_IM          1919 non-null int64\n",
        "RELJCT1_IM         1919 non-null int64\n",
        "RELJCT2_IM         1919 non-null int64\n",
        "LGTCON_IM          1919 non-null int64\n",
        "WEATHR_IM          1919 non-null int64\n",
        "ALCHL_IM           1919 non-null int64\n",
        "LAND_USE           1919 non-null int64\n",
        "VE_TOTAL           1919 non-null int64\n",
        "PEDS               1919 non-null int64\n",
        "PERMVIT            1919 non-null int64\n",
        "PERNOTMVIT         1919 non-null int64\n",
        "DAY_WEEK           1919 non-null int64\n",
        "ALCOHOL            1919 non-null int64\n",
        "RELJCT1            1919 non-null int64\n",
        "RELJCT2            1919 non-null int64\n",
        "TYP_INT            1919 non-null int64\n",
        "WRK_ZONE           1919 non-null int64\n",
        "REL_ROAD           1919 non-null int64\n",
        "LGT_COND           1919 non-null int64\n",
        "WEATHER1           1919 non-null int64\n",
        "WEATHER2           1919 non-null int64\n",
        "WEATHER            1919 non-null int64\n",
        "INT_HWY            1919 non-null int64\n",
        "CF1                1919 non-null int64\n",
        "CF2                1919 non-null int64\n",
        "CF3                1919 non-null int64\n",
        "DRIVER_AGE         1919 non-null int64\n",
        "DRIVER_SEX         1919 non-null int64\n",
        "DRIVER_DRUGS       1919 non-null int64\n",
        "dtypes: float64(1), int64(94)"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>7. Merge the GES Safety Equipment data with out dataframe</font></p>\n",
      "<p>Note: This step is performed after the cleaning in step 6 so that the safety information collected in both year 1 and year 2 can potentially be used in the model.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def addSafetyEQ(df, safetyDf):\n",
      "    \"\"\"df = the dataframe the safety columns are to be added to\n",
      "    safetyDf = the dataframe containing the GES safety equipment data\n",
      "    \n",
      "    =========\n",
      "    Returns the modified df\"\"\"\n",
      "    \n",
      "    cols = [\"MSAFEQMT{}\".format(x) for x in np.sort(safetyDf.MSAFEQMT.unique().tolist())] #Create the new column names\n",
      "    makeNewColumns(df, cols, [0]) #Add the new columns to the dataframe and initialize them with zero\n",
      "    df = df.apply(fillSafetyColumns, axis = 1, args = (safetyDf, cols)) #fill in the new columns\n",
      "    return df\n",
      "\n",
      "def fillSafetyColumns(ser, safetyDf, cols):\n",
      "    \"\"\"ser = a row from the dataframe having its safety columns added and populated.\n",
      "    safetyDf = the dataframe containing the GES safety equipment data\n",
      "    cols = the list of column names for the safety columns\"\"\"\n",
      "    \n",
      "    records = safetyDf[(safetyDf.CASENUM == ser[\"CASENUM\"]) & (safetyDf.PER_NO == ser[\"PER_NO\"])] #Isolate the relevant records\n",
      "    positive = records.MSAFEQMT.unique().tolist() #Find out which safety equipment codes are associated with this person\n",
      "    for col in cols: #Iterate through the safety columns\n",
      "        if int(col[-1]) in positive: #Check if this column's safety equipment code is associated with this person\n",
      "            ser[col] = 1 #If so, change this column's value to 1.\n",
      "    return ser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allButCategorySplit = addSafetyEQ(fullyMerged, safetyEQ)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4>8. Save the cleaned data</font></p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allButCategorySplit.to_csv(\"allButCategorySplit_20{}.csv\".format(YEAR))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4 color=\"blue\">Remaining:</font></p>\n",
      "\n",
      "<p><strong><u>Data Cleaning:</u></strong>\n",
      "    <ul><li>Go through all columns, and note the ones that are numeric vs categorical. Most will be categorical.</li>\n",
      "        <li>For the categorical columns which will actually be used in the classifier (for e.g., excluding things like the case number), convert each one into multiple columns, one per category.</li>\n",
      "        <li>Consider adding new columns which are dummy variables representing combinations of some of the categories in the categorical variables.</li>\n",
      "    </ul>\n",
      "</p>\n",
      "\n",
      "<p><strong><u>Modeling:</u></strong>\n",
      "    <ul><li>Use forward selection to build the logistic regression model?</li>\n",
      "        <li>Partition the data into a holdout and validation set (perhaps maintaining the ratio of serious and or fatal injuries to non-serious injuries). \n",
      "            <ol><li>Build the decision tree. Do an \"optimal\" pruning according to validation set accuracy.</li>\n",
      "                <li>Build a max depth tree under the idea of having at least 10 people per parameter in the final hybrid CART-Logistic Regression.</li>\n",
      "            </ol>\n",
      "        </li>\n",
      "        <li>Perform two(?) more logistic regression building efforts, this time with the decision tree dummy variables (from the optimally pruned tree in one, and from the max depth tree in the other) purposefully always included in the models.</li>\n",
      "    </ul>\n",
      "</p>\n",
      "\n",
      "<p><strong><u>Testing:</u></strong>\n",
      "    <ul><li>Prepare the data from the second year, in the same way the data for the first year was prepared.</li>\n",
      "        <li>Compute and store the prediction accuracies of the pure logit model, the \"optimally\" pruned tree, the \"max depth tree\", and the hybrid CART-Logit models using the second year of data.</li>\n",
      "        <li>Compute and store the sum of squared partial residuals for the pure logit model, the \"optimally\" pruned tree, the \"max depth tree\", and the hybrid CART-Logit models using their predictions for the second year of data.</li>\n",
      "    </ul>\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}