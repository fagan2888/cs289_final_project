{
 "metadata": {
  "name": "naiveDesignMatrix.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Create Naive Design Matrix and Unpruned Decision Trees"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Use the patsy library to create a design matrix that can be used for decision trees and for logistic regressions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np, pandas as pd, patsy, matplotlib.pyplot as plt, cPickle, treepredict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read in the saved dataframe where missing values were converted to 9999\n",
      "basicTransform = pd.read_pickle(\"dataframes/noMissingValueDf.pkl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basicTransform.groupby(\"MSAFEQMT9\").size()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "MSAFEQMT9\n",
        "0            1402\n",
        "1              25\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columnFrame = pd.read_csv(\"categoricalColumns_MissingValues.csv\") #Read in the csv of column names and additional info\n",
      "columnFrame.fillna(\"\", inplace = True) #Fill all null cells with an empty string\n",
      "columnFrame.DontKnow = columnFrame.DontKnow.astype(str) #Make the \"DontKnow\" column have type \"string\".\n",
      "columnFrame.index = columnFrame.Cols #Set the column names from basicTransform, which are unique, as the index.\n",
      "print columnFrame.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Index: 90 entries, CASENUM to MSAFEQMT9\n",
        "Data columns (total 6 columns):\n",
        "Cols              90 non-null object\n",
        "Categorical       90 non-null int64\n",
        "DontKnow          90 non-null object\n",
        "ImputedVersion    90 non-null object\n",
        "4Tree             90 non-null int64\n",
        "4Logit            90 non-null object\n",
        "dtypes: int64(2), object(4)None\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_design_string(df, columnDf):\n",
      "    \"\"\"df = the dataframe the design string for patsy is to be made from.\n",
      "    df should be the dataframe without missing values\n",
      "    columnDf = the dataframe describing which variables should be included \n",
      "    in the design matrix and which variables are categorical.\n",
      "    ==========\n",
      "    Returns a string to be used to create the patsy design matrix\"\"\"\n",
      "    dStringList = [] #Initialize an empty list to hold the terms that will make up the patsy formula for the design matrix\n",
      "    for col in df: #Iterate over each column in basicTransform\n",
      "        if columnDf.loc[col,\"4Tree\"] == 1: #Check if the column is one which I want to include in the decision tree\n",
      "            if columnDf.loc[col, \"Categorical\"] == 1: #Check if the column is a categorical variable.\n",
      "                if 9999 in df[col].values: #If the column has missing values, make that the reference.\n",
      "                    #Add the patsy string for the above conditions to the list\n",
      "                    dStringList.append(\"C({}, Treatment(9999))\".format(col)) \n",
      "                else\n",
      "                    if \"MSAFE\" not in col: #Account for the fact that MSAFEQMT columns have already been separated by categories\n",
      "                        #If not a MSAFEQMT column, separate the categorical variable in patsy\n",
      "                        dStringList.append(\"C({})\".format(col))\n",
      "                    else:\n",
      "                        #If it's an MSAFEQMT column, add it as is\n",
      "                        dStringList.append(\"{}\".format(col))\n",
      "            elif columnDf.loc[col, \"Categorical\"] == 0: #If the column is not categorical, add it directly to the patsy formula\n",
      "                dStringList.append(\"{}\".format(col))\n",
      "    dStringList.remove(\"C(INJ_SEV)\") #Don't use injury severity, use a 0-1 variable that represents serious injury or not, SER_INJ\n",
      "    dStringList.remove(\"MSAFEQMT8\") #Make \"Not Reported\" and \"Unknown if Used\" the 'unknown'/9999 reference category.\n",
      "    dStringList.remove(\"MSAFEQMT9\") #Make \"Not Reported\" and \"Unknown if Used\" the 'unknown'/9999 reference category.\n",
      "    dStringList.append(\"SER_INJ\") #Add SER_INJ to the list of variables to be used in the patsy design matrix\n",
      "    dString = \" + \".join(dStringList) #Create the patsy formula string\n",
      "    dStringList #Show the variables that will be included in the design matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dStringList = [] #Initialize an empty list to hold the terms that will make up the patsy formula for the design matrix\n",
      "for col in basicTransform: #Iterate over each column in basicTransform\n",
      "    if columnFrame.loc[col,\"4Tree\"] == 1: #Check if the column is one which I want to include in the decision tree\n",
      "        if columnFrame.loc[col, \"Categorical\"] == 1: #Check if the column is a categorical variable.\n",
      "            if 9999 in basicTransform[col].values: #If the column has missing values, make that the reference.\n",
      "                dStringList.append(\"C({}, Treatment(9999))\".format(col)) #Add the patsy string 4 the above conditions to the list\n",
      "            else:\n",
      "                if \"MSAFE\" not in col: #Account for the fact that the MSAFEQMT columns have already been turned into categories\n",
      "                    dStringList.append(\"C({})\".format(col)) #If not an MSAFEQMT column, separate the categorical variable in patsy\n",
      "                else:\n",
      "                    dStringList.append(\"{}\".format(col)) #If it's an MSAFEQMT column, add it as is\n",
      "        elif columnFrame.loc[col, \"Categorical\"] == 0: #If the column is not categorical, add it directly to the patsy formula\n",
      "            dStringList.append(\"{}\".format(col))\n",
      "dStringList.remove(\"C(INJ_SEV)\") #Don't use injury severity, use a 0-1 variable that represents serious injury or not, SER_INJ\n",
      "dStringList.remove(\"MSAFEQMT8\") #Make \"Not Reported\" and \"Unknown if Used\" the 'unknown'/9999 reference category.\n",
      "dStringList.remove(\"MSAFEQMT9\") #Make \"Not Reported\" and \"Unknown if Used\" the 'unknown'/9999 reference category.\n",
      "dStringList.append(\"SER_INJ\") #Add SER_INJ to the list of variables to be used in the patsy design matrix\n",
      "dString = \" + \".join(dStringList) #Create the patsy formula string\n",
      "dStringList #Show the variables that will be included in the design matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "['C(REGION)',\n",
        " 'C(MONTH)',\n",
        " 'C(DRUGS, Treatment(9999))',\n",
        " 'C(LOCATION, Treatment(9999))',\n",
        " 'C(SEX_IM)',\n",
        " 'C(PERALCH_IM)',\n",
        " 'AGE_IM',\n",
        " 'NUMOCCS',\n",
        " 'C(TOW_VEH_vehDup, Treatment(9999))',\n",
        " 'C(GVWR, Treatment(9999))',\n",
        " 'C(V_CONFIG, Treatment(9999))',\n",
        " 'C(CARGO_BT, Treatment(9999))',\n",
        " 'C(BUS_USE, Treatment(9999))',\n",
        " 'C(SPEC_USE_vehDup, Treatment(9999))',\n",
        " 'C(EMER_USE_vehDup, Treatment(9999))',\n",
        " 'TRAV_SP',\n",
        " 'C(IMPACT2_vehDup, Treatment(9999))',\n",
        " 'C(SPEEDREL, Treatment(9999))',\n",
        " 'C(VTRAFWAY, Treatment(9999))',\n",
        " 'VNUM_LAN',\n",
        " 'VSPD_LIM',\n",
        " 'C(VALIGN, Treatment(9999))',\n",
        " 'C(VPROFILE, Treatment(9999))',\n",
        " 'C(VSURCOND, Treatment(9999))',\n",
        " 'C(VTRAFCON, Treatment(9999))',\n",
        " 'C(VTCONT_F, Treatment(9999))',\n",
        " 'C(P_CRASH2, Treatment(9999))',\n",
        " 'C(P_CRASH3, Treatment(9999))',\n",
        " 'C(PCRASH4, Treatment(9999))',\n",
        " 'C(PCRASH5, Treatment(9999))',\n",
        " 'C(ACC_TYPE, Treatment(9999))',\n",
        " 'C(HITRUN_IM)',\n",
        " 'C(BDYTYP_IM)',\n",
        " 'C(IMPACT1_IM)',\n",
        " 'C(PCRASH1_IM)',\n",
        " 'C(V_ALCH_IM)',\n",
        " 'C(LAND_USE, Treatment(9999))',\n",
        " 'PEDS',\n",
        " 'PERMVIT',\n",
        " 'PERNOTMVIT',\n",
        " 'C(ALCOHOL, Treatment(9999))',\n",
        " 'C(TYP_INT, Treatment(9999))',\n",
        " 'C(WRK_ZONE)',\n",
        " 'C(REL_ROAD, Treatment(9999))',\n",
        " 'C(WKDY_IM)',\n",
        " 'C(HOUR_IM)',\n",
        " 'C(RELJCT1_IM)',\n",
        " 'C(RELJCT2_IM)',\n",
        " 'C(LGTCON_IM)',\n",
        " 'C(WEATHR_IM)',\n",
        " 'C(ALCHL_IM)',\n",
        " 'DRIVER_AGE',\n",
        " 'C(DRIVER_SEX, Treatment(9999))',\n",
        " 'C(DRIVER_DRUGS, Treatment(9999))',\n",
        " 'MSAFEQMT1',\n",
        " 'MSAFEQMT2',\n",
        " 'MSAFEQMT3',\n",
        " 'MSAFEQMT4',\n",
        " 'MSAFEQMT5',\n",
        " 'MSAFEQMT7',\n",
        " 'SER_INJ']"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dString #Show what the patsy formula string looks like."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "'C(REGION) + C(MONTH) + C(DRUGS, Treatment(9999)) + C(LOCATION, Treatment(9999)) + C(SEX_IM) + C(PERALCH_IM) + AGE_IM + NUMOCCS + C(TOW_VEH_vehDup, Treatment(9999)) + C(GVWR, Treatment(9999)) + C(V_CONFIG, Treatment(9999)) + C(CARGO_BT, Treatment(9999)) + C(BUS_USE, Treatment(9999)) + C(SPEC_USE_vehDup, Treatment(9999)) + C(EMER_USE_vehDup, Treatment(9999)) + TRAV_SP + C(IMPACT2_vehDup, Treatment(9999)) + C(SPEEDREL, Treatment(9999)) + C(VTRAFWAY, Treatment(9999)) + VNUM_LAN + VSPD_LIM + C(VALIGN, Treatment(9999)) + C(VPROFILE, Treatment(9999)) + C(VSURCOND, Treatment(9999)) + C(VTRAFCON, Treatment(9999)) + C(VTCONT_F, Treatment(9999)) + C(P_CRASH2, Treatment(9999)) + C(P_CRASH3, Treatment(9999)) + C(PCRASH4, Treatment(9999)) + C(PCRASH5, Treatment(9999)) + C(ACC_TYPE, Treatment(9999)) + C(HITRUN_IM) + C(BDYTYP_IM) + C(IMPACT1_IM) + C(PCRASH1_IM) + C(V_ALCH_IM) + C(LAND_USE, Treatment(9999)) + PEDS + PERMVIT + PERNOTMVIT + C(ALCOHOL, Treatment(9999)) + C(TYP_INT, Treatment(9999)) + C(WRK_ZONE) + C(REL_ROAD, Treatment(9999)) + C(WKDY_IM) + C(HOUR_IM) + C(RELJCT1_IM) + C(RELJCT2_IM) + C(LGTCON_IM) + C(WEATHR_IM) + C(ALCHL_IM) + DRIVER_AGE + C(DRIVER_SEX, Treatment(9999)) + C(DRIVER_DRUGS, Treatment(9999)) + MSAFEQMT1 + MSAFEQMT2 + MSAFEQMT3 + MSAFEQMT4 + MSAFEQMT5 + MSAFEQMT7 + SER_INJ'"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basicTransform[\"SER_INJ\"] = 0 #initialize the SER_INJ variable to be zero\n",
      "for ind, row in basicTransform.iterrows(): #Iterate over the rows of basicTransform\n",
      "    if row[\"INJ_SEV\"] in [3,4]: #Check if the row corresponds to a person suffering an incapacitating or fatal injury\n",
      "        basicTransform.loc[ind, \"SER_INJ\"] = 1 #Change their SER_INJ value to 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dmatrix = patsy.dmatrix(dString, data = basicTransform) #Create the patsy design matrix\n",
      "design4Tree = pd.DataFrame(dmatrix, columns = dmatrix.design_info.column_names, dtype=int) #Make the d-matrix a pandas dataframe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "design4Tree.to_pickle(\"dataframes/design_DF_4Tree.pkl\") #Save the design matrix dataframe for the decision tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Split the training data into validation and training sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ppl = design4Tree.index.tolist() #Create a list of the indices in design4Tree\n",
      "np.random.shuffle(ppl) #Shuffle the list of indices in design4Tree\n",
      "\n",
      "stop = int(0.2 * len(ppl) + 0.5) #Round 20% of the length of the dataframe to the nearest whole number\n",
      "\n",
      "validationInd = ppl[:stop] #Take approximately 20% of the shuffled list of indices to be indices in the validation set.\n",
      "trainingInd = ppl[stop:] #Take the remaining approximately 80% of the shuffled list of indices to be the training set.\n",
      "\n",
      "tSet = design4Tree.loc[trainingInd] #Create the training set\n",
      "vSet = design4Tree.loc[validationInd] #Create the validation set\n",
      "\n",
      "#========\n",
      "#Comments below are what I would use if doing a stratified sample to create the validation set.\n",
      "\n",
      "# bHurt = design4Tree[design4Tree[\"SER_INJ\"] == 1].index.tolist() #List of indices for badly hurt individuals\n",
      "# sHurt = design4Tree[design4Tree[\"SER_INJ\"] == 0].index.tolist() #List of indices for slightly hurt individuals\n",
      "\n",
      "#np.random.shuffle(bHurt) #Shuffle the list of badly hurt people\n",
      "#np.random.shuffle(sHurt) #Shuffle the list of slightly hurt people\n",
      "\n",
      "# bStop = int(0.2 * len(bHurt) + 0.5) #20 percent of the badly hurt individuals\n",
      "# sStop = int(0.2 * len(sHurt) + 0.5) #20 percent of the slightly hurt individuals\n",
      "\n",
      "#trainingInd = bHurt[bStop:] + sHurt[sStop:]\n",
      "#validationInd = bHurt[:bStop] + sHurt[:sStop]\n",
      "\n",
      "# tSet = design4Tree.loc[trainingInd] #training set\n",
      "# vSet = design4Tree.loc[validationInd] #validation set\n",
      "\n",
      "#=========="
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tSet.to_pickle(\"trainingSet_DF_naiveTree.pkl\")\n",
      "vSet.to_pickle(\"validationSet_DF_naiveTree.pkl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Build and save unpruned decision trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unprunedNaiveTree = treepredict.buildTreePandas(design4Tree, \"SER_INJ\") #Grow a full depth tree on the entire dataset\n",
      "\n",
      "with open(\"unpruned_NaiveTree_fullData.pkl\", \"wb\") as f: #save the full dataset's unpruned, naive tree\n",
      "    cPickle.dump(unprunedNaiveTree, f)\n",
      "\n",
      "rel_feats = treepredict.searchFeats(unprunedNaiveTree) #Features used by the full dataset's unpruned, naive tree\n",
      "print \"While the full dataset contained {} variables, the decision tree only used {} variables\".format(len(design4Tree.columns),\n",
      "                                                                                                        len(rel_feats))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "While the full dataset contained 331 variables, the decision tree only used 105 variables\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unpruned_tset_NaiveTree = treepredict.buildTreePandas(tSet, \"SER_INJ\") #Grow a full depth tree on the training data\n",
      "\n",
      "with open(\"unpruned_NaiveTree_trainingSet.pkl\", \"wb\") as f: #save the training set's unpruned, naive tree\n",
      "    cPickle.dump(unpruned_tset_NaiveTree, f)\n",
      "\n",
      "rel_feats2 = treepredict.searchFeats(unpruned_tset_NaiveTree) #Features used by the training set's unpruned, naive tree\n",
      "print \"While the full dataset contained 332 variables, the decision tree only used {} variables\".format(len(tSet.columns),\n",
      "                                                                                                        len(rel_feats2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "While the full dataset contained 331 variables, the decision tree only used 140 variables\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Test the accuracy of the unpruned tree's predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#make a function to asses the accuracy of one's tree.\n",
      "def testAccuracy(testSet, tree, classes, res):\n",
      "    preds = [] #Initialize an empty list to hold the predictions\n",
      "    actual = np.array(testSet[res]) #Get the actual array of class labels\n",
      "    for ind, row in testSet.iterrows(): #Iterate over all rows of the test data\n",
      "        preds.append(treepredict.predictWithTree(row, tree, classes)) #Make predictions on the test data\n",
      "    preds = np.array(preds) #make predictions an array\n",
      "    numWrong = 0 #Initialize the number of wrong predictions\n",
      "    for i in range(len(preds)): #Iterate over all of the predictions and actual labels\n",
      "        if preds[i] != actual[i]: #Check to see if the predictions are not the same.\n",
      "            numWrong += 1 #If the predictions are not the same, increment numWrong by one\n",
      "    return 1 - float(numWrong)/len(testSet) #Return the accuracy of the tree's predictions."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"The accuracy of the unpruned training set tree on the validation set is {} (decimal)\".format(testAccuracy(vSet,\n",
      "                                                                                                           unpruned_tset_NaiveTree,\n",
      "                                                                                                           [0,1],\"SER_INJ\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 97,
       "text": [
        "'The accuracy of the unpruned training set tree on the validation set is 0.656140350877 (decimal)'"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vSet.groupby(\"SER_INJ\").size() #Look at the counts of observations within each injury class in the validation set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "SER_INJ\n",
        "0          223\n",
        "1           62\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs = tSet.groupby(\"SER_INJ\").size() * 1.0 / len(tSet)\n",
      "probs #Look at the probabilities of being in each injury class in the test set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "SER_INJ\n",
        "0          0.794221\n",
        "1          0.205779\n",
        "dtype: float64"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vSet) * probs #Calculate the # of ppl in the validation set within each injury class based on the training set proportions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 109,
       "text": [
        "SER_INJ\n",
        "0          226.35289\n",
        "1           58.64711\n",
        "dtype: float64"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "226 + 59 == np.sum(vSet.groupby(\"SER_INJ\").size())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 112,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create a list of zeros and ones in the same proportion as the training data, but of the size of the test data.\n",
      "randomPreds = [0 for x in range(226)] + [1 for x in range(59)]\n",
      "np.random.shuffle(randomPreds) #randomize the list created above\n",
      "#Calculate the accuracy of this random set of predictions\n",
      "diff = randomPreds - np.array(vSet.SER_INJ) #Take the difference in the two columns\n",
      "numWrong = np.sum(np.absolute(diff)) #Sum up all the |1's| or |-1's| since the predictions are either 0 or 1.\n",
      "1 - float(numWrong)/len(vSet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "0.6736842105263158"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size = 4>End Result:</font></p>\n",
      "<p>The randomly created set of predictions performed better than the predictions of the unpruned tree (~67% accuracy compared to ~66%)...<br>\n",
      "Clearly pruning needs to occur. There is massive overfitting.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><font size=4 color=\"blue\">Remaining:</font></p>\n",
      "\n",
      "<p><strong><u>Data Cleaning:</u></strong>\n",
      "    <ul><li>Go through all columns in the categoricalColumns_MissingValues.csv file to double check the values I put in as missing value/other codes.</li>\n",
      "        <li>Consider adding new columns which are dummy variables representing combinations of some of the categories in the categorical variables.</li>\n",
      "    </ul>\n",
      "</p>\n",
      "\n",
      "<p><strong><u>Modeling:</u></strong>\n",
      "    <ul><li>Use forward selection to build the logistic regression model?</li>\n",
      "        <li><strike>Build the decision tree.</strike> \n",
      "            <ol><li> Do an \"optimal\" pruning according to validation set accuracy.</li>\n",
      "                <li>Build a max depth/min leaf-size tree under the idea of having at least 10 people per parameter in the final hybrid CART-Logistic Regression.</li>\n",
      "            </ol>\n",
      "        </li>\n",
      "        <li>Perform two(?) more logistic regression building efforts, this time with the decision tree dummy variables (from the optimally pruned tree in one, and from the max depth tree in the other) purposefully always included in the models.</li>\n",
      "    </ul>\n",
      "</p>\n",
      "\n",
      "<p><strong><u>Testing:</u></strong>\n",
      "    <ul><li>Go through the 2012 GES user manual to make sure that our code is approapriately dealing with any column name changes or code changes from 2011 to 2012.</li>\n",
      "        <li>Compute and store the prediction accuracies of the pure logit model, the \"optimally\" pruned tree, the \"max depth tree\", and the hybrid CART-Logit models using the second year of data.</li>\n",
      "        <li>Compute and store the sum of squared partial residuals for the pure logit model, the \"optimally\" pruned tree, the \"max depth tree\", and the hybrid CART-Logit models using their predictions for the second year of data.</li>\n",
      "    </ul>\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}